{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# from torchvision.io import read_image\n",
    "import re\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import wfdb.processing\n",
    "import scipy\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_segments(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "# Przykład zapisu w metodzie `__init__` klasy MIT_BIH_Arythmia:\n",
    "\n",
    "def extract_segment_with_padding(z, k, N):\n",
    "    # Rozmiar segmentu to 2N + 1\n",
    "    start_idx = k - N\n",
    "    end_idx = k + N + 1  # Indeks końcowy +1, ponieważ Python używa wykluczającego indeksu\n",
    "    \n",
    "    # Upewnij się, że start_idx i end_idx mieszczą się w granicach tablicy\n",
    "    if start_idx < 0:\n",
    "        # Jeśli start_idx jest poza zakresem, dopełnij na początku\n",
    "        padding_left = np.median(z[:end_idx])  # Wypełniamy medianą\n",
    "        segment = np.concatenate([np.full(-start_idx, padding_left), z[:end_idx]])\n",
    "    elif end_idx > len(z):\n",
    "        # Jeśli end_idx jest poza zakresem, dopełnij na końcu\n",
    "        padding_right = np.median(z[start_idx:])  # Wypełniamy medianą\n",
    "        segment = np.concatenate([z[start_idx:], np.full(end_idx - len(z), padding_right)])\n",
    "    else:\n",
    "        # Normalny przypadek, kiedy zakres mieści się w tablicy\n",
    "        segment = z[start_idx:end_idx]\n",
    "    \n",
    "    return segment\n",
    "\n",
    "def find_nearest_qrs_index(annotation_sample, qrs_inds):\n",
    "    # Find the index in qrs_inds that is closest to annotation_sample\n",
    "    distances = np.abs(qrs_inds - annotation_sample)\n",
    "    nearest_idx = np.argmin(distances)  # Get the index of the minimum distance\n",
    "    return qrs_inds[nearest_idx]\n",
    "\n",
    "class MIT_BIH_Arythmia(Dataset):\n",
    "    def __init__(self,N, M, dataset_dir = 'Datasets/files/', fs = 10, filename = \"ltafdb_0_39.json\"):\n",
    "        \"\"\"\n",
    "        n - number of samples of orginal signal resampled to fs, interval [-n,n]\n",
    "        m - qrs times, interval [-m,m]\n",
    "        \"\"\"\n",
    "        ecg_list = []\n",
    "        exclusion_lst = [str(i) for i in range(10,11)]\n",
    "        # print(os.listdir(dataset_dir))\n",
    "        for file in os.listdir(dataset_dir):\n",
    "            name = re.match(r'^(.*\\d\\d+)\\.atr$', file)\n",
    "            if name:\n",
    "                if name.group(1) in exclusion_lst:\n",
    "                    print(name.group(1))\n",
    "                    record = wfdb.rdsamp(f\"{dataset_dir}{name.group(1)}\") \n",
    "                    annotation = wfdb.rdann(f\"{dataset_dir}{name.group(1)}\", 'atr')\n",
    "                    signal = record[0][:,0]\n",
    "                    fs_original = record[1][\"fs\"]\n",
    "                    num_samples_target = int(signal.shape[0] * fs / fs_original)\n",
    "                    resampled_signal = scipy.signal.resample(signal, num_samples_target)\n",
    "                    annotation_times_resampled = (annotation.sample * fs) / fs_original\n",
    "                    resampled_annotation = wfdb.Annotation('atr',annotation.symbol,annotation_times_resampled.astype(int),aux_note=annotation.aux_note)\n",
    "                    ecg_list.append({\"name\": name.group(1),\"rec\" : resampled_signal, \"ann\" : resampled_annotation})\n",
    "        self.samples_list = []\n",
    "        self.label_list = []\n",
    "        self.qrs_samples = []\n",
    "        for dic in ecg_list:\n",
    "            print(dic[\"name\"])\n",
    "            # xqrs = wfdb.processing.XQRS(sig=dic[\"rec\"], fs=fs)\n",
    "            # xqrs.detect()\n",
    "            # qrs_inds = xqrs.qrs_inds\n",
    "            for n,i in enumerate(dic[\"ann\"].sample):\n",
    "                self.label_list.append(1 if dic[\"ann\"].aux_note[n] == '(AFIB' else 0)\n",
    "                self.samples_list.append(list(extract_segment_with_padding(dic[\"rec\"], dic[\"ann\"].sample[n],N)))\n",
    "                # nearest_qrs_idx = find_nearest_qrs_index(dic[\"ann\"].sample[n], qrs_inds)\n",
    "                # self.qrs_samples.append(list(extract_segment_with_padding(qrs_inds,nearest_qrs_idx,M)))\n",
    "        data = {\n",
    "            'samples_list': self.samples_list,  # This would work if the segments are simple numeric lists\n",
    "            'label_list': self.label_list,\n",
    "            'qrs_samples': self.qrs_samples\n",
    "        }\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        for dic in ecg_list:\n",
    "            output_filename = f\"{dic['name']}_data.pkl\"\n",
    "            save_segments({\n",
    "                'rec': dic[\"rec\"],\n",
    "                'ann': {\n",
    "                    'sample': dic[\"ann\"].sample.tolist(),\n",
    "                    'aux_note': dic[\"ann\"].aux_note\n",
    "                }\n",
    "            }, output_filename)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples_list[idx], self.label_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "ds = MIT_BIH_Arythmia(100,5,fs=100,dataset_dir='physionet.org/files/ltafdb/1.0.0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.3\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import wfdb.processing\n",
    "import scipy\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def extract_segment_with_padding(z, k, N):\n",
    "    start_idx = k - N\n",
    "    end_idx = k + N + 1\n",
    "    \n",
    "    if start_idx < 0:\n",
    "        padding_left = np.median(z[:end_idx])\n",
    "        segment = np.concatenate([np.full(-start_idx, padding_left), z[:end_idx]])\n",
    "    elif end_idx > len(z):\n",
    "        padding_right = np.median(z[start_idx:])\n",
    "        segment = np.concatenate([z[start_idx:], np.full(end_idx - len(z), padding_right)])\n",
    "    else:\n",
    "        segment = z[start_idx:end_idx]\n",
    "    \n",
    "    return segment\n",
    "\n",
    "class MIT_BIH_Arythmia(Dataset):\n",
    "    def __init__(self, N, M, dataset_dir='Datasets/files/', fs=10, output_dir=\"processed_data/\"):\n",
    "        \"\"\"\n",
    "        N - number of samples of the original signal resampled to fs, interval [-N, N]\n",
    "        M - interval around QRS times [-M, M]\n",
    "        \"\"\"\n",
    "        # Upewnij się, że folder na przetworzone dane istnieje\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        exclusion_lst = [str(i) for i in range(10, 11)]\n",
    "        self.cumulative_histogram = []\n",
    "        start_idx = 0\n",
    "\n",
    "        for file in os.listdir(dataset_dir):\n",
    "            name = re.match(r'^(.*\\d\\d+)\\.atr$', file)\n",
    "            if name and name.group(1) in exclusion_lst:\n",
    "                print(f\"Przetwarzanie: {name.group(1)}\")\n",
    "                record = wfdb.rdsamp(f\"{dataset_dir}{name.group(1)}\")\n",
    "                annotation = wfdb.rdann(f\"{dataset_dir}{name.group(1)}\", 'atr')\n",
    "                \n",
    "                signal = record[0][:, 0]\n",
    "                fs_original = record[1][\"fs\"]\n",
    "                num_samples_target = int(signal.shape[0] * fs / fs_original)\n",
    "                resampled_signal = scipy.signal.resample(signal, num_samples_target)\n",
    "                annotation_times_resampled = (annotation.sample * fs) / fs_original\n",
    "\n",
    "                # Przygotowanie danych do zapisu\n",
    "                data = {\n",
    "                    \"rec\": resampled_signal,\n",
    "                    \"ann\": {\n",
    "                        \"sample\": annotation_times_resampled.astype(int).tolist(),\n",
    "                        \"aux_note\": annotation.aux_note\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Zapis do pliku pickle\n",
    "                output_filename = os.path.join(output_dir, f\"{name.group(1)}.pkl\")\n",
    "                with open(output_filename, 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "\n",
    "                # Uaktualnienie histogramu skumulowanego\n",
    "                num_samples = len(data[\"ann\"][\"sample\"])\n",
    "                self.cumulative_histogram.append((start_idx, start_idx + num_samples, output_filename))\n",
    "                start_idx += num_samples\n",
    "        \n",
    "        # Zapis histogramu do łatwego odnajdywania danych\n",
    "        histogram_path = os.path.join(output_dir, \"cumulative_histogram.pkl\")\n",
    "        with open(histogram_path, 'wb') as f:\n",
    "            pickle.dump(self.cumulative_histogram, f)\n",
    "\n",
    "        print(\"Przetwarzanie zakończone. Dane zapisane w:\", output_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_histogram[-1][1]  # Ostatni indeks w histogramie\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Znajdź plik i indeks lokalny\n",
    "        for start, end, filename in self.cumulative_histogram:\n",
    "            if start <= idx < end:\n",
    "                local_idx = idx - start\n",
    "                break\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range\")\n",
    "\n",
    "        # Wczytaj dane z odpowiedniego pliku\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        # Wyciągnij segment i etykietę\n",
    "        rec = data[\"rec\"]\n",
    "        sample_idx = data[\"ann\"][\"sample\"][local_idx]\n",
    "        aux_note = data[\"ann\"][\"aux_note\"][local_idx]\n",
    "\n",
    "        segment = extract_segment_with_padding(rec, sample_idx, self.cumulative_histogram[0][2])  # Zakładamy, że N jest w histogramie\n",
    "        label = 1 if aux_note == '(AFIB' else 0\n",
    "\n",
    "        return segment, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przetwarzanie: 10\n",
      "Przetwarzanie zakończone. Dane zapisane w: processed_data/\n"
     ]
    }
   ],
   "source": [
    "ds = MIT_BIH_Arythmia(100,5,fs=100,dataset_dir='physionet.org/files/ltafdb/1.0.0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import pickle\n",
    "import scipy\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def extract_segment_with_padding(z, k, N):\n",
    "    start_idx = k - N\n",
    "    end_idx = k + N + 1\n",
    "    if start_idx < 0:\n",
    "        padding_left = np.median(z[:end_idx])\n",
    "        segment = np.concatenate([np.full(-start_idx, padding_left), z[:end_idx]])\n",
    "    elif end_idx > len(z):\n",
    "        padding_right = np.median(z[start_idx:])\n",
    "        segment = np.concatenate([z[start_idx:], np.full(end_idx - len(z), padding_right)])\n",
    "    else:\n",
    "        segment = z[start_idx:end_idx]\n",
    "    return segment\n",
    "\n",
    "class MIT_BIH_Arythmia(Dataset):\n",
    "    def __init__(self, N, M, dataset_dir='Datasets/files/', fs=10, output_dir=\"processed_data/\", histogram_path=None):\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        if histogram_path and os.path.exists(histogram_path):\n",
    "            with open(histogram_path, 'rb') as f:\n",
    "                self.cumulative_histogram = pickle.load(f)\n",
    "            print(\"Załadowano histogram:\", histogram_path)\n",
    "        else:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            exclusion_lst = [str(i) for i in range(10, 11)]\n",
    "            self.cumulative_histogram = []\n",
    "            start_idx = 0\n",
    "            for file in os.listdir(dataset_dir):\n",
    "                name = re.match(r'^(.*\\d\\d+)\\.atr$', file)\n",
    "                if name and name.group(1) not in exclusion_lst:\n",
    "                    print(f\"Przetwarzanie: {name.group(1)}\")\n",
    "                    record = wfdb.rdsamp(f\"{dataset_dir}{name.group(1)}\")\n",
    "                    annotation = wfdb.rdann(f\"{dataset_dir}{name.group(1)}\", 'atr')\n",
    "                    signal = record[0][:, 0]\n",
    "                    fs_original = record[1][\"fs\"]\n",
    "                    num_samples_target = int(signal.shape[0] * fs / fs_original)\n",
    "                    resampled_signal = scipy.signal.resample(signal, num_samples_target)\n",
    "                    annotation_times_resampled = (annotation.sample * fs) / fs_original\n",
    "                    data = {\n",
    "                        \"rec\": resampled_signal,\n",
    "                        \"ann\": {\n",
    "                            \"sample\": annotation_times_resampled.astype(int).tolist(),\n",
    "                            \"aux_note\": annotation.aux_note\n",
    "                        }\n",
    "                    }\n",
    "                    output_filename = os.path.join(output_dir, f\"{name.group(1)}.pkl\")\n",
    "                    with open(output_filename, 'wb') as f:\n",
    "                        pickle.dump(data, f)\n",
    "                    num_samples = len(data[\"ann\"][\"sample\"])\n",
    "                    self.cumulative_histogram.append((start_idx, start_idx + num_samples, output_filename))\n",
    "                    start_idx += num_samples\n",
    "            histogram_path = os.path.join(output_dir, \"cumulative_histogram.pkl\")\n",
    "            with open(histogram_path, 'wb') as f:\n",
    "                pickle.dump(self.cumulative_histogram, f)\n",
    "            print(\"Przetwarzanie zakończone. Dane zapisane w:\", output_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_histogram[-1][1]\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     for start, end, filename in self.cumulative_histogram:\n",
    "    #         if start <= idx < end:\n",
    "    #             local_idx = idx - start\n",
    "    #             break\n",
    "    #     else:\n",
    "    #         raise IndexError(\"Index out of range\")\n",
    "    #     with open(filename, 'rb') as f:\n",
    "    #         data = pickle.load(f)\n",
    "    #     rec = data[\"rec\"]\n",
    "    #     sample_idx = data[\"ann\"][\"sample\"][local_idx]\n",
    "    #     aux_note = data[\"ann\"][\"aux_note\"][local_idx]\n",
    "    #     segment = extract_segment_with_padding(rec, sample_idx, self.N)\n",
    "    #     label = 1 if aux_note == '(AFIB' else 0\n",
    "    #     return segment, label\n",
    "    def __getitem__(self, idx):\n",
    "    # Przejdź przez histogram skumulowany, aby znaleźć odpowiedni plik i zakres indeksów\n",
    "        for start, end, filename in self.cumulative_histogram:\n",
    "            if start <= idx < end:\n",
    "                local_idx = idx - start  # Oblicz indeks lokalny w danym pliku\n",
    "                break\n",
    "        else:\n",
    "            raise IndexError(\"Index out of range\")  # Jeśli nie znajdziesz odpowiedniego zakresu, zgłoś błąd\n",
    "        \n",
    "        # Załaduj dane z odpowiedniego pliku\n",
    "        with open(filename, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        # Pobierz sygnał EKG i informacje o annotacjach\n",
    "        rec = data[\"rec\"]\n",
    "        sample_idx = data[\"ann\"][\"sample\"][local_idx]  # Indeks próbki w danym pliku\n",
    "        aux_note = data[\"ann\"][\"aux_note\"][local_idx]  # Etykieta (np. AFIB lub NORMAL)\n",
    "        \n",
    "        # Wyciąć odpowiedni segment EKG wokół punktu annotacji\n",
    "        segment = extract_segment_with_padding(rec, sample_idx, self.N)\n",
    "        \n",
    "        # Ustal etykietę: 1 dla AFIB, 0 dla NORMAL\n",
    "        label = 1 if aux_note == '(AFIB' else 0\n",
    "        \n",
    "        return segment, label\n",
    "\n",
    "    def count_afibs(self):\n",
    "        afib_count = 0\n",
    "        for start, end, filename in self.cumulative_histogram:\n",
    "            # Załaduj dane z pliku\n",
    "            with open(filename, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            # Sprawdź wszystkie etykiety i policz AFIB\n",
    "            for aux_note in data[\"ann\"][\"aux_note\"]:\n",
    "                if \"(AFIB\" in aux_note:\n",
    "                    afib_count += 1\n",
    "        return afib_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przetwarzanie: 101\n",
      "Przetwarzanie: 25\n",
      "Przetwarzanie: 19\n",
      "Przetwarzanie: 116\n",
      "Przetwarzanie: 42\n",
      "Przetwarzanie: 110\n",
      "Przetwarzanie: 58\n",
      "Przetwarzanie: 112\n",
      "Przetwarzanie: 205\n",
      "Przetwarzanie: 122\n",
      "Przetwarzanie: 20\n",
      "Przetwarzanie: 24\n",
      "Przetwarzanie: 06\n",
      "Przetwarzanie: 45\n",
      "Przetwarzanie: 17\n",
      "Przetwarzanie: 05\n",
      "Przetwarzanie: 48\n",
      "Przetwarzanie: 111\n",
      "Przetwarzanie: 35\n",
      "Przetwarzanie: 200\n",
      "Przetwarzanie: 64\n",
      "Przetwarzanie: 204\n",
      "Przetwarzanie: 00\n",
      "Przetwarzanie: 53\n",
      "Przetwarzanie: 72\n",
      "Przetwarzanie: 69\n",
      "Przetwarzanie: 201\n",
      "Przetwarzanie: 113\n",
      "Przetwarzanie: 65\n",
      "Przetwarzanie: 21\n",
      "Przetwarzanie: 121\n",
      "Przetwarzanie: 30\n",
      "Przetwarzanie: 75\n",
      "Przetwarzanie: 60\n",
      "Przetwarzanie: 22\n",
      "Przetwarzanie: 34\n",
      "Przetwarzanie: 100\n",
      "Przetwarzanie: 33\n",
      "Przetwarzanie: 74\n",
      "Przetwarzanie: 114\n",
      "Przetwarzanie: 13\n",
      "Przetwarzanie: 102\n",
      "Przetwarzanie: 55\n",
      "Przetwarzanie: 115\n",
      "Przetwarzanie: 43\n",
      "Przetwarzanie: 03\n",
      "Przetwarzanie: 117\n",
      "Przetwarzanie: 38\n",
      "Przetwarzanie: 118\n",
      "Przetwarzanie: 37\n",
      "Przetwarzanie: 23\n",
      "Przetwarzanie: 49\n",
      "Przetwarzanie: 203\n",
      "Przetwarzanie: 62\n",
      "Przetwarzanie: 207\n",
      "Przetwarzanie: 47\n",
      "Przetwarzanie: 18\n",
      "Przetwarzanie: 51\n",
      "Przetwarzanie: 202\n",
      "Przetwarzanie: 26\n",
      "Przetwarzanie: 16\n",
      "Przetwarzanie: 103\n",
      "Przetwarzanie: 68\n",
      "Przetwarzanie: 208\n",
      "Przetwarzanie: 70\n",
      "Przetwarzanie: 12\n",
      "Przetwarzanie: 56\n",
      "Przetwarzanie: 206\n",
      "Przetwarzanie: 44\n",
      "Przetwarzanie: 71\n",
      "Przetwarzanie: 120\n",
      "Przetwarzanie: 15\n",
      "Przetwarzanie: 39\n",
      "Przetwarzanie: 07\n",
      "Przetwarzanie: 28\n",
      "Przetwarzanie: 105\n",
      "Przetwarzanie: 119\n",
      "Przetwarzanie: 54\n",
      "Przetwarzanie: 08\n",
      "Przetwarzanie: 01\n",
      "Przetwarzanie: 104\n",
      "Przetwarzanie: 32\n",
      "Przetwarzanie: 11\n",
      "Przetwarzanie zakończone. Dane zapisane w: processed_data/\n"
     ]
    }
   ],
   "source": [
    "ds = MIT_BIH_Arythmia(100,5,fs=100,dataset_dir='physionet.org/files/ltafdb/1.0.0/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.80920419e-01, -1.87886019e-01, -1.71094582e-01, -1.75422381e-01,\n",
       "        -1.21075154e-01, -2.11214783e-02,  3.26984099e-01,  4.09966285e-01,\n",
       "         1.08096163e-01, -5.36241415e-02, -1.34585308e-01, -1.67034761e-01,\n",
       "        -1.85654787e-01, -1.84693876e-01, -1.96240578e-01, -1.94296828e-01,\n",
       "        -2.01947176e-01, -1.89263046e-01, -1.59897901e-01, -1.75522421e-01,\n",
       "        -1.56267666e-01, -1.63724781e-01, -1.17240218e-01, -1.00394377e-01,\n",
       "        -6.35989552e-02, -5.65785919e-02,  8.05510398e-04,  5.21243145e-02,\n",
       "         8.81517655e-02,  1.43967229e-01,  1.86545818e-01,  2.45467441e-01,\n",
       "         2.64598292e-01,  2.83127739e-01,  2.68716921e-01,  2.51028287e-01,\n",
       "         1.95043746e-01,  1.15671605e-01,  5.47469707e-02, -2.59107093e-02,\n",
       "        -4.13080978e-02, -9.06302535e-02, -1.08586610e-01, -1.26123670e-01,\n",
       "        -1.40026424e-01, -1.29399617e-01, -1.40629107e-01, -1.47295787e-01,\n",
       "        -1.61834673e-01, -1.27796788e-01, -1.26506303e-01, -1.22751849e-01,\n",
       "        -1.30008213e-01, -1.26917850e-01, -1.12922699e-01, -1.20108693e-01,\n",
       "        -1.10506302e-01, -1.21504332e-01, -1.14223799e-01, -1.08324597e-01,\n",
       "        -1.11511942e-01, -1.17719490e-01, -1.27433120e-01, -1.09010182e-01,\n",
       "        -1.20851625e-01, -1.16231148e-01, -1.30550806e-01, -1.22410741e-01,\n",
       "        -1.07845722e-01, -1.19440682e-01, -1.14162791e-01, -1.29112270e-01,\n",
       "        -9.97576320e-02, -1.08337634e-01, -1.06248768e-01, -1.10557787e-01,\n",
       "        -1.03304530e-01, -9.45551770e-02, -1.03429476e-01, -1.06631983e-01,\n",
       "        -1.12449109e-01, -8.42225558e-02, -8.93209549e-02, -5.24561395e-02,\n",
       "        -4.17117740e-02, -1.12718627e-02,  3.67449045e-02,  5.33832780e-02,\n",
       "         3.02936029e-02,  4.18517884e-03, -2.65160959e-02, -8.21673761e-02,\n",
       "        -8.61446980e-02, -9.90072141e-02, -8.72723551e-02, -7.05947752e-02,\n",
       "        -6.69349817e-02, -4.08272944e-02,  1.05772871e-04,  1.55536352e-01,\n",
       "         4.47833352e-01,  4.77359668e-01,  2.35982770e-01,  1.30917738e-01,\n",
       "         3.41853049e-02, -2.69022653e-02, -8.29225009e-02, -8.98165649e-02,\n",
       "        -8.84845922e-02, -1.00711172e-01, -9.70960358e-02, -1.03653438e-01,\n",
       "        -5.78476407e-02, -6.40751580e-02, -4.91490830e-02, -4.03784938e-02,\n",
       "        -2.29689496e-02,  1.30147931e-02,  3.52034782e-02,  6.19109650e-02,\n",
       "         9.57511019e-02,  1.66375505e-01,  1.91847851e-01,  2.65445893e-01,\n",
       "         2.77935413e-01,  3.52413171e-01,  3.67750288e-01,  3.90824736e-01,\n",
       "         3.52031086e-01,  3.50354502e-01,  2.85887504e-01,  2.20215987e-01,\n",
       "         1.29026813e-01,  6.53577882e-02,  2.86800679e-02, -1.31047823e-02,\n",
       "        -4.57680365e-02, -7.11239456e-02, -6.00662007e-02, -7.61740042e-02,\n",
       "        -7.54447363e-02, -9.41365350e-02, -7.64888088e-02, -8.03977956e-02,\n",
       "        -7.78803630e-02, -9.05533312e-02, -7.92910798e-02, -8.18734096e-02,\n",
       "        -8.37548744e-02, -1.00184682e-01, -8.90501901e-02, -8.59727089e-02,\n",
       "        -1.11558349e-01, -1.12369131e-01, -1.19967998e-01, -1.09134929e-01,\n",
       "        -1.30329360e-01, -1.24510335e-01, -1.48045618e-01, -1.25713673e-01,\n",
       "        -1.32700834e-01, -1.41766292e-01, -1.65452173e-01, -1.33155819e-01,\n",
       "        -1.51208709e-01, -1.56746628e-01, -1.70353304e-01, -1.53210680e-01,\n",
       "        -1.56037061e-01, -1.62628179e-01, -1.72150276e-01, -1.57456292e-01,\n",
       "        -1.45552590e-01, -1.59890139e-01, -1.54365866e-01, -1.30393570e-01,\n",
       "        -8.26780037e-02, -8.47096093e-02, -4.83757807e-02, -3.18409307e-02,\n",
       "        -9.50895720e-03, -5.99169225e-02, -1.02525133e-01, -1.58107857e-01,\n",
       "        -1.60238137e-01, -1.72047167e-01, -1.65442344e-01, -1.47422067e-01,\n",
       "        -1.19899866e-01, -1.20269159e-01, -8.33626361e-02,  2.45037802e-02,\n",
       "         3.93237995e-01,  4.63891253e-01,  1.59099098e-01, -2.09290294e-02,\n",
       "        -1.25754313e-01, -1.61439431e-01, -1.88908131e-01, -1.99843292e-01,\n",
       "        -2.25957558e-01]),\n",
       " 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Załadowano histogram: processed_data/cumulative_histogram.pkl\n",
      "Batch 0:\n",
      "  - Rozmiar segmentów: torch.Size([32, 201])\n",
      "  - Rozmiar etykiet: torch.Size([32])\n",
      "Batch 1:\n",
      "  - Rozmiar segmentów: torch.Size([32, 201])\n",
      "  - Rozmiar etykiet: torch.Size([32])\n",
      "Batch 2:\n",
      "  - Rozmiar segmentów: torch.Size([32, 201])\n",
      "  - Rozmiar etykiet: torch.Size([32])\n",
      "Batch 3:\n",
      "  - Rozmiar segmentów: torch.Size([32, 201])\n",
      "  - Rozmiar etykiet: torch.Size([32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Iteracja po danych\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (segments, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Rozmiar segmentów: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegments\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dnn/dnn_ecg/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/dnn/dnn_ecg/env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/dnn/dnn_ecg/env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/dnn/dnn_ecg/env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[11], line 93\u001b[0m, in \u001b[0;36mMIT_BIH_Arythmia.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Załaduj dane z odpowiedniego pliku\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 93\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Pobierz sygnał EKG i informacje o annotacjach\u001b[39;00m\n\u001b[1;32m     96\u001b[0m rec \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Stwórz dataset\n",
    "# dataset = MIT_BIH_Arythmia(N=100, M=50, histogram_path=\"processed_data/cumulative_histogram.pkl\")\n",
    "\n",
    "# # Stwórz DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Iteracja po danych\n",
    "# for batch_idx, (segments, labels) in enumerate(dataloader):\n",
    "#     print(f\"Batch {batch_idx}:\")\n",
    "#     print(f\"  - Rozmiar segmentów: {segments.shape}\")\n",
    "#     print(f\"  - Rozmiar etykiet: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7278"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.count_afibs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 99533, 'processed_data/101.pkl'), (99533, 210189, 'processed_data/25.pkl'), (210189, 318253, 'processed_data/19.pkl'), (318253, 431527, 'processed_data/116.pkl'), (431527, 545974, 'processed_data/42.pkl'), (545974, 651435, 'processed_data/110.pkl'), (651435, 761540, 'processed_data/58.pkl'), (761540, 879828, 'processed_data/112.pkl'), (879828, 996330, 'processed_data/205.pkl'), (996330, 1087214, 'processed_data/122.pkl'), (1087214, 1195213, 'processed_data/20.pkl'), (1195213, 1292639, 'processed_data/24.pkl'), (1292639, 1397578, 'processed_data/06.pkl'), (1397578, 1493732, 'processed_data/45.pkl'), (1493732, 1631793, 'processed_data/17.pkl'), (1631793, 1744756, 'processed_data/05.pkl'), (1744756, 1891543, 'processed_data/48.pkl'), (1891543, 1994169, 'processed_data/111.pkl'), (1994169, 2083335, 'processed_data/35.pkl'), (2083335, 2165990, 'processed_data/200.pkl'), (2165990, 2285771, 'processed_data/64.pkl'), (2285771, 2422559, 'processed_data/204.pkl'), (2422559, 2528896, 'processed_data/00.pkl'), (2528896, 2578665, 'processed_data/53.pkl'), (2578665, 2723062, 'processed_data/72.pkl'), (2723062, 2864587, 'processed_data/69.pkl'), (2864587, 2952066, 'processed_data/201.pkl'), (2952066, 3064880, 'processed_data/113.pkl'), (3064880, 3160980, 'processed_data/65.pkl'), (3160980, 3243158, 'processed_data/21.pkl'), (3243158, 3323866, 'processed_data/121.pkl'), (3323866, 3355056, 'processed_data/30.pkl'), (3355056, 3481829, 'processed_data/75.pkl'), (3481829, 3598648, 'processed_data/60.pkl'), (3598648, 3726855, 'processed_data/22.pkl'), (3726855, 3862031, 'processed_data/34.pkl'), (3862031, 3955177, 'processed_data/100.pkl'), (3955177, 4076571, 'processed_data/33.pkl'), (4076571, 4171682, 'processed_data/74.pkl'), (4171682, 4284225, 'processed_data/114.pkl'), (4284225, 4353729, 'processed_data/13.pkl'), (4353729, 4468204, 'processed_data/102.pkl'), (4468204, 4575115, 'processed_data/55.pkl'), (4575115, 4684537, 'processed_data/115.pkl'), (4684537, 4791572, 'processed_data/43.pkl'), (4791572, 4875838, 'processed_data/03.pkl'), (4875838, 4975927, 'processed_data/117.pkl'), (4975927, 5059732, 'processed_data/38.pkl'), (5059732, 5184905, 'processed_data/118.pkl'), (5184905, 5252486, 'processed_data/37.pkl'), (5252486, 5354980, 'processed_data/23.pkl'), (5354980, 5434884, 'processed_data/49.pkl'), (5434884, 5508735, 'processed_data/203.pkl'), (5508735, 5632357, 'processed_data/62.pkl'), (5632357, 5763652, 'processed_data/207.pkl'), (5763652, 5854592, 'processed_data/47.pkl'), (5854592, 5996106, 'processed_data/18.pkl'), (5996106, 6091226, 'processed_data/51.pkl'), (6091226, 6236851, 'processed_data/202.pkl'), (6236851, 6306277, 'processed_data/26.pkl'), (6306277, 6430442, 'processed_data/16.pkl'), (6430442, 6586399, 'processed_data/103.pkl'), (6586399, 6771208, 'processed_data/68.pkl'), (6771208, 6891959, 'processed_data/208.pkl'), (6891959, 7020296, 'processed_data/70.pkl'), (7020296, 7137441, 'processed_data/12.pkl'), (7137441, 7269579, 'processed_data/56.pkl'), (7269579, 7391002, 'processed_data/206.pkl'), (7391002, 7538462, 'processed_data/44.pkl'), (7538462, 7663665, 'processed_data/71.pkl'), (7663665, 7774212, 'processed_data/120.pkl'), (7774212, 7861479, 'processed_data/15.pkl'), (7861479, 7975721, 'processed_data/39.pkl'), (7975721, 8085346, 'processed_data/07.pkl'), (8085346, 8183940, 'processed_data/28.pkl'), (8183940, 8254115, 'processed_data/105.pkl'), (8254115, 8353632, 'processed_data/119.pkl'), (8353632, 8471832, 'processed_data/54.pkl'), (8471832, 8580199, 'processed_data/08.pkl'), (8580199, 8671708, 'processed_data/01.pkl'), (8671708, 8755922, 'processed_data/104.pkl'), (8755922, 8846225, 'processed_data/32.pkl'), (8846225, 8948826, 'processed_data/11.pkl')]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Załaduj dane z pliku pickle\n",
    "with open('processed_data/cumulative_histogram.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Sprawdź pierwsze elementy\n",
    "print(data)  # Powinna to być lista lub słownik z segmentami i etykietami\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m afib_samples\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Pobierz 5 próbek AFIB\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m afib_samples \u001b[38;5;241m=\u001b[39m \u001b[43mextract_afib_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m, in \u001b[0;36mextract_afib_samples\u001b[0;34m(dataset, num_samples)\u001b[0m\n\u001b[1;32m      2\u001b[0m afib_samples \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[0;32m----> 5\u001b[0m     segment, label \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Sprawdź, czy etykieta to AFIB (1 oznacza AFIB)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[15], line 93\u001b[0m, in \u001b[0;36mMIT_BIH_Arythmia.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# Załaduj dane z odpowiedniego pliku\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 93\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Pobierz sygnał EKG i informacje o annotacjach\u001b[39;00m\n\u001b[1;32m     96\u001b[0m rec \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_afib_samples(dataset, num_samples=5):\n",
    "    afib_samples = []\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        segment, label = dataset[idx]\n",
    "        \n",
    "        # Sprawdź, czy etykieta to AFIB (1 oznacza AFIB)\n",
    "        if label == 1:\n",
    "            afib_samples.append(segment)\n",
    "        \n",
    "        # Jeśli już mamy wystarczającą liczbę próbek AFIB, zakończ\n",
    "        if len(afib_samples) >= num_samples:\n",
    "            break\n",
    "    \n",
    "    return afib_samples\n",
    "\n",
    "# Pobierz 5 próbek AFIB\n",
    "afib_samples = extract_afib_samples(ds, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_afib_samples(afib_samples):\n",
    "    for i, sample in enumerate(afib_samples):\n",
    "        plt.subplot(1, len(afib_samples), i + 1)\n",
    "        plt.plot(sample)\n",
    "        plt.title(f\"AFIB Sample {i+1}\")\n",
    "        plt.xlabel(\"Czas\")\n",
    "        plt.ylabel(\"Amplituda\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Wyświetl próbek\n",
    "plot_afib_samples(afib_samples)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
